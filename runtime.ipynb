{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runtime measurements\n",
    "\n",
    "## Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import random\n",
    "import glob\n",
    "import yaml\n",
    "import timeit\n",
    "import math\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import KFold, train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "sns.set(font_scale=1, style='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedModel(BaseEstimator):\n",
    "    \"\"\"\n",
    "    sklearn model of the fixed model, \n",
    "    always allocating/predicting a fixed, pre-defined amount of resources.\n",
    "    For use in sklearn functions like k-fold CV\n",
    "    \"\"\"\n",
    "    def __init__(self, fixed_value):\n",
    "        self.fixed_value = fixed_value\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"There's nothing to fit here\"\"\"\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Always 'predict' the specified fixed value\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        return [self.fixed_value for _ in range(n_samples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for processing and simplifying the dataset\n",
    "def select_and_rename(df, mapping):\n",
    "    \"\"\"\n",
    "    Helper: Selects columns of df using the keys of the mapping dict.\n",
    "    It renames the columns to the values of the mappings dict.\n",
    "    \"\"\"\n",
    "    # select subset of columns\n",
    "    dff = df[list(mapping.keys())]\n",
    "    # rename \n",
    "    for k, v in mapping.items():\n",
    "        dff.rename(columns={k: v}, inplace=True)\n",
    "    return dff\n",
    "\n",
    "def replaceSize(df):\n",
    "    df[\"size\"] = df[\"size\"].str.replace(\"ab -c 1 -t 60 -n 99999999 -e /tngbench_share/ab_dist.csv -s 60 -k -i http://20.0.0.254:8888/\", \"small\")\n",
    "    df[\"size\"] = df[\"size\"].str.replace(\"ab -c 1 -t 60 -n 99999999 -e /tngbench_share/ab_dist.csv -s 60 -k http://20.0.0.254:8888/bunny.mp4\", \"big\")\n",
    "    df[\"size\"] = df[\"size\"].str.replace(\"ab -c 1 -t 60 -n 99999999 -e /tngbench_share/ab_dist.csv -s 60 -k -i -X 20.0.0.254:3128 http://40.0.0.254:80/\", \"small\")\n",
    "    df[\"size\"] = df[\"size\"].str.replace(\"ab -c 1 -t 60 -n 99999999 -e /tngbench_share/ab_dist.csv -s 60 -k -X 20.0.0.254:3128 http://40.0.0.254:80/bunny.mp4\", \"big\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\stefan\\git-repos\\work\\ai\\venv\\lib\\site-packages\\pandas\\core\\frame.py:3781: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  return super(DataFrame, self).rename(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Load data from path\n",
    "web1 = pd.read_csv(\"vnf_data/csv_experiments_WEB1.csv\")\n",
    "web3 = pd.read_csv(\"vnf_data/csv_experiments_WEB3.csv\")\n",
    "\n",
    "# do processing, renaming and selection\n",
    "mapping = {\n",
    "    \"param__func__mp.input__cmd_start\": \"size\",\n",
    "    \"metric__mp.input.vdu01.0__ab_transfer_rate_kbyte_per_second\": \"Max. throughput [kB/s]\",\n",
    "}\n",
    "\n",
    "mapping01 = mapping.copy()\n",
    "mapping01[\"param__func__de.upb.lb-nginx.0.1__cpu_bw\"] = \"CPU\"\n",
    "mapping01[\"param__func__de.upb.lb-nginx.0.1__mem_max\"] = \"Memory\"\n",
    "\n",
    "mapping03 = mapping.copy()\n",
    "mapping03[\"param__func__de.upb.px-squid.0.1__cpu_bw\"] = \"CPU\"\n",
    "mapping03[\"param__func__de.upb.px-squid.0.1__mem_max\"] = \"Memory\"\n",
    "\n",
    "web1 = select_and_rename(web1, mapping01)\n",
    "web3 = select_and_rename(web3, mapping03)\n",
    "\n",
    "web1 = replaceSize(web1)\n",
    "web3 = replaceSize(web3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select sub-datasets with small and large flows\n",
    "# and with specific memory\n",
    "mem = 128\n",
    "\n",
    "web1_small = web1.loc[(web1[\"size\"] == \"small\") & (web1[\"Memory\"] == mem)]\n",
    "web1_small = web1_small[[\"Max. throughput [kB/s]\", \"CPU\"]]\n",
    "web3_small = web3.loc[(web3[\"size\"] == \"small\")  & (web3[\"Memory\"] == mem)]\n",
    "web3_small = web3_small[[\"Max. throughput [kB/s]\", \"CPU\"]]\n",
    "\n",
    "# add 20 \"measurements\" at 0 CPU and throuhgput\n",
    "num_measures = 20\n",
    "measures = [0 for _ in range(num_measures)]\n",
    "web1_small = web1_small.append(pd.DataFrame({'Max. throughput [kB/s]': measures, 'CPU': measures}), ignore_index=True)\n",
    "web3_small = web3_small.append(pd.DataFrame({'Max. throughput [kB/s]': measures, 'CPU': measures}), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data):\n",
    "    \"\"\"Split return data\"\"\"\n",
    "    X = data[['Max. throughput [kB/s]']]\n",
    "    y = data['CPU']\n",
    "    X = X.fillna(X.median())\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit_transform(X)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def barplot(times, labels, ylabel='Time [s]', filename=None):\n",
    "    sns.set(font_scale=1.2, style='white')\n",
    "    assert len(times) == len(labels)\n",
    "    \n",
    "    times_mean = [np.array(t).mean() for t in times]\n",
    "    print(times_mean)\n",
    "    times_std = [np.array(t).std() for t in times]\n",
    "    \n",
    "    x = np.arange(len(labels))\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.bar(x, times_mean, yerr=times_std, capsize=5, color='gray')\n",
    "    \n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    \n",
    "    if filename is not None:\n",
    "        fig.savefig(f'plots/{filename}.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparison_barplot(times_squid, times_nginx, labels, ylabel='Time [s]', filename=None):\n",
    "    sns.set(font_scale=1.2, style='white')\n",
    "    assert len(times_squid) == len(times_nginx) == len(labels)\n",
    "    \n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.35\n",
    "\n",
    "    # plot\n",
    "    fig, ax = plt.subplots(figsize = (8, 5))\n",
    "    plt.bar(x - width/2, times_squid, width, color='gray', label='Squid')\n",
    "    plt.bar(x + width/2, times_nginx, width, color='lightgray', label='Nginx')\n",
    "        \n",
    "    # labels\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.legend()\n",
    "    \n",
    "    if filename is not None:\n",
    "        fig.savefig(f'plots/{filename}.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "X_nginx, y_nginx = prepare_data(web1_small)\n",
    "X_squid, y_squid = prepare_data(web3_small)\n",
    "\n",
    "X = X_nginx\n",
    "y = y_nginx\n",
    "vnf_name = 'nginx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate synthetic data\n",
    "def gen_benchmark(cpu, coeff1=1, coeff2=1):\n",
    "    \"\"\"Generate max throughput for given CPU time according to log function\"\"\"\n",
    "    return coeff1 * math.log2(1 + coeff2 * cpu)\n",
    "\n",
    "def synthetic_benchmark(n):\n",
    "    \"\"\"Generate n synthetic benchmark results for CPU time 0-100%\"\"\"\n",
    "    cpu_list = [random.random() for _ in range(n)]\n",
    "    thr_list = [gen_benchmark(cpu, coeff2=100) for cpu in cpu_list]\n",
    "    X = pd.DataFrame(data={'Throughput': thr_list})\n",
    "    y = np.array(cpu_list)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_times(models, X, y, scaler):\n",
    "    # measure training times\n",
    "    times = []\n",
    "    for model in models:\n",
    "        print(type(model).__name__)\n",
    "        X_scaled = scaler.transform(X)\n",
    "        t = %timeit -o model.fit(X, y)\n",
    "        # save the best time, which is the most comparable and reproducible\n",
    "        times.append(t)\n",
    "    return times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression\n",
      "12.1 ms ± 538 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Ridge\n",
      "5.54 ms ± 282 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "SVR\n"
     ]
    }
   ],
   "source": [
    "labels = ['Linear', 'Ridge', 'SVR', 'Forest', 'Boosting', 'MLP', 'Fixed']\n",
    "models = [LinearRegression(), Ridge(), SVR(verbose=True), RandomForestRegressor(), \n",
    "          GradientBoostingRegressor(), MLPRegressor(max_iter=1500), \n",
    "          FixedModel(fixed_value=0.8)]\n",
    "model_names = [type(model).__name__ for model in models]\n",
    "models = [joblib.load(f'ml_models/{vnf_name}/{name}.joblib') for name in model_names]\n",
    "scaler = joblib.load(f'ml_models/{vnf_name}/scaler.joblib')\n",
    "\n",
    "# X_rand = pd.DataFrame(data={'Rand max. throughput': [random.randrange(0, 3000) for _ in range(100000)]})\n",
    "# y_rand = np.array([random.random() for _ in range(100000)])\n",
    "\n",
    "X_synth, y_synth = synthetic_benchmark(200000)\n",
    "\n",
    "times = training_times(models, X_synth, y_synth, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "# timeit runs 7 runs with variable number of loops; calc time per loop for each run of each model\n",
    "all_times = [[i / t.loops for i in t.all_runs] for t in times]\n",
    "barplot(all_times, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained models and scaler\n",
    "# model_names = [type(model).__name__ for model in models]\n",
    "models = [joblib.load(f'ml_models/{vnf_name}/{name}.joblib') for name in model_names]\n",
    "scaler = joblib.load(f'ml_models/{vnf_name}/scaler.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time single prediction for rand input\n",
    "X_rand = pd.DataFrame(data={'Rand max. throughput': [random.randrange(0, 3000) for _ in range(1)]})\n",
    "times = []\n",
    "for model in models:\n",
    "    print(type(model).__name__)\n",
    "    X_scaled = scaler.transform(X_rand)\n",
    "    t = %timeit -o model.predict(X_scaled)\n",
    "    # save the best time, which is the most comparable and reproducible\n",
    "    times.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_times_ms = [[i * 1000.0 / t.loops for i in t.all_runs] for t in times]\n",
    "barplot(all_times_ms, labels, ylabel=\"Prediction time [ms]\", filename='prediction_times')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    print(model)\n",
    "    \n",
    "# reasons for relatively high prediction time of random forrest - in contrast to gradient boosting \n",
    "# both tree-based ensemble methods; both n_estimators=10 --> same number of decision trees\n",
    "# unlimited max_depth of forest; max_depth 3 for gradient boosting --> deeper decision trees\n",
    "\n",
    "# mlp is fast because it's pretty shallow (128,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placement times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_cpu(node_res):\n",
    "    \"\"\"Return sum of CPU resources allocated to all nodes\"\"\"\n",
    "    cpu = sum([v['cpu'] for v in node_res])\n",
    "    return cpu\n",
    "\n",
    "def read_placement(placement, df_data, flow_dr=250):\n",
    "    \"\"\"Read placement dict and write it to df_data. Then return.\"\"\"\n",
    "    df_data['num_flows'].append(placement['input']['num_flows'])\n",
    "    df_data['num_sources'].append(placement['input']['num_sources'])\n",
    "    df_data['source_dr'].append(placement['input']['num_flows'] * flow_dr)\n",
    "    df_data['num_instances'].append(placement['metrics']['num_instances'])\n",
    "    df_data['max_e2e_delay'].append(placement['metrics']['max_endToEnd_delay'])\n",
    "    df_data['total_delay'].append(placement['metrics']['total_delay'])\n",
    "    df_data['runtime'].append(placement['metrics']['runtime'])\n",
    "    df_data['total_cpu'].append(sum_cpu(placement['placement']['alloc_node_res']))\n",
    "    return df_data\n",
    "\n",
    "def read_results(results):\n",
    "    \"\"\"Read result files matching the pattern and return df containing their metrics\"\"\"\n",
    "    data = {'num_sources' : [], 'num_flows': [], 'source_dr': [], 'num_instances': [], \n",
    "            'max_e2e_delay': [], 'total_delay': [], 'runtime': [], 'total_cpu': []}\n",
    "\n",
    "    # iterate through result files\n",
    "    for res in glob.glob(results):\n",
    "        # open and save metrics of interest\n",
    "        with open(res, 'r') as f:\n",
    "            placement = yaml.load(f)\n",
    "            data = read_placement(placement, data)\n",
    "\n",
    "    return pd.DataFrame(data).sort_values(by=['num_flows'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read results\n",
    "dataset = 'web_data'\n",
    "sources = 'runtime'\n",
    "results = f'placement_data/{dataset}/{sources}/'\n",
    "\n",
    "df_true = read_results(results + 'true/*.yaml')\n",
    "df_fixed = read_results(results + 'fixed/*.yaml')\n",
    "df_linear = read_results(results + 'linear/*.yaml')\n",
    "df_boost = read_results(results + 'boosting/*.yaml')\n",
    "df_svr = read_results(results + 'svr/*.yaml')\n",
    "df_ml = read_results(results + 'ml/*.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "placement_times = [df_fixed['runtime'].mean(), df_linear['runtime'].mean(), df_ml['runtime'].mean()]\n",
    "labels = ['Fixed', 'Linear', 'SVR+Boosting']\n",
    "\n",
    "placement_times\n",
    "\n",
    "barplot(placement_times, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
